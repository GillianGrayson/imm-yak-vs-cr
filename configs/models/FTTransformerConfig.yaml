_module_src: models.ft_transformer
_model_name: FTTransformerModel
_backbone_name: FTTransformerBackbone
_config_name: FTTransformerConfig

task: classification
head: LinearHead
head_config:
  layers: ""
  activation: ReLU
  dropout: 0.1
  use_batch_norm: false
  initialization: xavier
learning_rate: 1e-3
loss: CrossEntropyLoss
metrics:
  - accuracy
  - f1_score
  - precision
  - recall
  - specificity
  - cohen_kappa
  - auroc
metrics_prob_input:
  - True
  - True
  - True
  - True
  - True
  - True
  - True
metrics_params:
  - {task: multiclass, num_classes: 2, average: 'macro'}
  - {task: multiclass, num_classes: 2, average: 'macro'}
  - {task: multiclass, num_classes: 2, average: 'macro'}
  - {task: multiclass, num_classes: 2, average: 'macro'}
  - {task: multiclass, num_classes: 2, average: 'macro'}
  - {task: multiclass, num_classes: 2}
  - {task: multiclass, num_classes: 2, average: 'macro'}
target_range: null
seed: 42

attn_feature_importance: false
num_heads: 8
num_attn_blocks: 6
attn_dropout: 0.1
add_norm_dropout: 0.1
ff_dropout: 0.1
ff_hidden_multiplier: 4
transformer_activation: GEGLU