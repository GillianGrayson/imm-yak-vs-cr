optimizer: Adam
optimizer_params: {}

#lr_scheduler: ReduceLROnPlateau
#lr_scheduler_params:
#  mode: min
#  factor: 0.1
#  patience: 10
#  threshold: 1e-4

lr_scheduler: CosineAnnealingWarmRestarts
lr_scheduler_params:
  T_0: 10
  T_mult: 1
  eta_min: 1e-5

#lr_scheduler: StepLR
#lr_scheduler_params:
#  step_size: 25
#  gamma: 0.1

lr_scheduler_monitor_metric: valid_loss